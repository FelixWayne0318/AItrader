#!/usr/bin/env python3
"""
Smart Commit Analyzer - æ™ºèƒ½æäº¤åˆ†æå™¨

è‡ªåŠ¨æ¼”è¿›çš„æäº¤åˆ†æç³»ç»Ÿï¼š
1. ä» git å†å²è‡ªåŠ¨æå–ä¿®å¤
2. AI ç”ŸæˆéªŒè¯è§„åˆ™
3. è§„åˆ™åº“è‡ªåŠ¨å¢é•¿
4. æ¯æ¬¡è¿è¡Œè‡ªåŠ¨æ£€æµ‹å›å½’

ä¸å†éœ€è¦æ‰‹åŠ¨ç»´æŠ¤è§„åˆ™åˆ—è¡¨ï¼
"""

import subprocess
import json
import os
import re
import hashlib
from pathlib import Path
from datetime import datetime
from typing import Optional

# è§„åˆ™å­˜å‚¨æ–‡ä»¶
RULES_FILE = Path(__file__).parent.parent / "configs" / "auto_generated_rules.json"

# æ–‡ä»¶ç§»åŠ¨æ£€æµ‹ï¼šå¸¸è§çš„æ–°ä½ç½®
FILE_SEARCH_PATHS = [
    "",           # åŸä½ç½® (ç›¸å¯¹äºé¡¹ç›®æ ¹ç›®å½•)
    "scripts/",   # è„šæœ¬ç›®å½•
    "tests/",     # æµ‹è¯•ç›®å½•
    "tools/",     # å·¥å…·ç›®å½•
    "docs/",      # æ–‡æ¡£ç›®å½•
]


def find_file(filepath: str) -> tuple[str, bool]:
    """
    æŸ¥æ‰¾æ–‡ä»¶ï¼Œå¦‚æœåœ¨åŸä½ç½®ä¸å­˜åœ¨ï¼Œå°è¯•åœ¨å…¶ä»–ç›®å½•æŸ¥æ‰¾
    è¿”å›: (å®é™…è·¯å¾„, æ˜¯å¦è¢«ç§»åŠ¨)
    """
    project_root = get_project_root()

    # å…ˆæ£€æŸ¥åŸä½ç½®
    if (project_root / filepath).exists():
        return filepath, False

    # æ–‡ä»¶ä¸åœ¨åŸä½ç½®ï¼Œå°è¯•å…¶ä»–ä½ç½®
    filename = Path(filepath).name

    for search_path in FILE_SEARCH_PATHS:
        new_path = search_path + filename
        if (project_root / new_path).exists():
            return new_path, True

    # è¿˜æ‰¾ä¸åˆ°ï¼Œè¿”å›åŸè·¯å¾„
    return filepath, False


def run_git(cmd: str) -> str:
    """æ‰§è¡Œ git å‘½ä»¤ (åœ¨é¡¹ç›®æ ¹ç›®å½•)"""
    result = subprocess.run(
        f"git {cmd}",
        shell=True,
        capture_output=True,
        text=True,
        cwd=get_project_root()
    )
    return result.stdout.strip()


def get_project_root() -> Path:
    """è·å–é¡¹ç›®æ ¹ç›®å½•"""
    return Path(__file__).parent.parent


def get_file_content(filepath: str) -> Optional[str]:
    """è¯»å–æ–‡ä»¶å†…å®¹ (ç›¸å¯¹äºé¡¹ç›®æ ¹ç›®å½•)"""
    try:
        full_path = get_project_root() / filepath
        if full_path.exists():
            return full_path.read_text(encoding='utf-8', errors='ignore')
    except Exception:
        pass
    return None


def load_rules() -> dict:
    """åŠ è½½å·²æœ‰è§„åˆ™"""
    if RULES_FILE.exists():
        try:
            return json.loads(RULES_FILE.read_text())
        except Exception:
            pass
    return {"rules": [], "metadata": {"created": datetime.now().isoformat()}}


def save_rules(rules: dict):
    """ä¿å­˜è§„åˆ™"""
    RULES_FILE.parent.mkdir(parents=True, exist_ok=True)
    rules["metadata"]["updated"] = datetime.now().isoformat()
    rules["metadata"]["count"] = len(rules["rules"])
    RULES_FILE.write_text(json.dumps(rules, indent=2, ensure_ascii=False))


def generate_rule_id(commit_hash: str, file_path: str) -> str:
    """ç”Ÿæˆå”¯ä¸€è§„åˆ™ ID"""
    content = f"{commit_hash}:{file_path}"
    return hashlib.md5(content.encode()).hexdigest()[:12]


def extract_fix_pattern(diff: str, file_path: str) -> Optional[dict]:
    """ä» diff ä¸­æå–ä¿®å¤æ¨¡å¼"""
    # æå–æ·»åŠ çš„è¡Œ (ä»¥ + å¼€å¤´ï¼Œä½†ä¸æ˜¯ +++)
    added_lines = []
    removed_lines = []

    for line in diff.split('\n'):
        if line.startswith('+') and not line.startswith('+++'):
            added_lines.append(line[1:].strip())
        elif line.startswith('-') and not line.startswith('---'):
            removed_lines.append(line[1:].strip())

    if not added_lines:
        return None

    # è¯†åˆ«å…³é”®æ¨¡å¼
    pattern = None
    pattern_type = "contains"

    # æ¨¡å¼1: import è¯­å¥å˜æ›´
    for line in added_lines:
        if line.startswith('from ') or line.startswith('import '):
            pattern = line
            pattern_type = "import"
            break

    # æ¨¡å¼2: å‡½æ•°/æ–¹æ³•å®šä¹‰
    if not pattern:
        for line in added_lines:
            if re.match(r'^\s*def \w+', line):
                pattern = re.search(r'def (\w+)', line).group(1)
                pattern_type = "function_exists"
                break

    # æ¨¡å¼3: å…³é”®ä»£ç ç‰‡æ®µ (é…ç½®ã€éªŒè¯ç­‰)
    if not pattern:
        keywords = ['if ', 'return ', '.get(', 'config', 'validate', 'check']
        for line in added_lines:
            for kw in keywords:
                if kw in line and len(line) > 10 and len(line) < 200:
                    pattern = line
                    pattern_type = "contains"
                    break
            if pattern:
                break

    # æ¨¡å¼4: å–æœ€é•¿çš„æœ‰æ„ä¹‰æ·»åŠ è¡Œ
    if not pattern:
        meaningful = [l for l in added_lines if len(l) > 15 and not l.startswith('#')]
        if meaningful:
            pattern = max(meaningful, key=len)[:150]
            pattern_type = "contains"

    if pattern:
        return {
            "pattern": pattern,
            "type": pattern_type,
            "added_count": len(added_lines),
            "removed_count": len(removed_lines)
        }

    return None


def analyze_commit_for_rules(commit_hash: str, message: str) -> list:
    """åˆ†æå•ä¸ªæäº¤ï¼Œç”Ÿæˆè§„åˆ™"""
    rules = []

    # è·å–è¯¥æäº¤ä¿®æ”¹çš„æ–‡ä»¶
    files = run_git(f"diff-tree --no-commit-id --name-only -r {commit_hash}").split('\n')
    files = [f for f in files if f.endswith('.py')]

    for file_path in files:
        if not file_path:
            continue

        # è·å–è¯¥æ–‡ä»¶çš„ diff
        diff = run_git(f"show {commit_hash} -- {file_path}")

        # æå–ä¿®å¤æ¨¡å¼
        fix_pattern = extract_fix_pattern(diff, file_path)

        if fix_pattern:
            rule_id = generate_rule_id(commit_hash, file_path)

            rule = {
                "id": rule_id,
                "commit": commit_hash[:7],
                "file": file_path,
                "message": message[:100],
                "pattern": fix_pattern["pattern"],
                "pattern_type": fix_pattern["type"],
                "created": datetime.now().isoformat(),
                "auto_generated": True
            }
            rules.append(rule)

    return rules


def scan_git_history(limit: int = 100) -> list:
    """æ‰«æ git å†å²ï¼Œæå–æ‰€æœ‰ä¿®å¤æäº¤"""
    # è·å–ä¿®å¤ç±»å‹çš„æäº¤
    log_output = run_git(f'log --oneline -n {limit} --grep="fix" --grep="Fix" --grep="ä¿®å¤" --grep="bugfix"')

    fix_commits = []
    for line in log_output.split('\n'):
        if not line:
            continue
        parts = line.split(' ', 1)
        if len(parts) == 2:
            fix_commits.append({
                "hash": parts[0],
                "message": parts[1]
            })

    # ä¹Ÿæ£€æŸ¥ conventional commits æ ¼å¼
    log_output2 = run_git(f'log --oneline -n {limit} --grep="^fix:"')
    for line in log_output2.split('\n'):
        if not line:
            continue
        parts = line.split(' ', 1)
        if len(parts) == 2:
            commit = {"hash": parts[0], "message": parts[1]}
            if commit not in fix_commits:
                fix_commits.append(commit)

    return fix_commits


def validate_rule(rule: dict, auto_fix_paths: bool = False) -> dict:
    """éªŒè¯å•æ¡è§„åˆ™"""
    file_path = rule.get("file")
    pattern = rule.get("pattern")
    pattern_type = rule.get("pattern_type", "contains")

    result = {
        "id": rule["id"],
        "file": file_path,
        "commit": rule.get("commit"),
        "status": "unknown",
        "message": "",
        "moved_to": None,
        "path_updated": False
    }

    # è·³è¿‡å·²åºŸå¼ƒçš„è§„åˆ™
    if rule.get("deprecated"):
        result["status"] = "skipped"
        result["message"] = f"Deprecated: {rule.get('deprecated_reason', 'No reason given')}"
        return result

    # æ£€æµ‹æ–‡ä»¶æ˜¯å¦è¢«ç§»åŠ¨
    actual_path, was_moved = find_file(file_path)

    if was_moved:
        result["moved_to"] = actual_path
        if auto_fix_paths:
            rule["file"] = actual_path
            result["path_updated"] = True
            result["message"] = f"Path updated: {file_path} â†’ {actual_path}"

    content = get_file_content(actual_path)

    if content is None:
        # æ–‡ä»¶ä¸å­˜åœ¨ - å¯èƒ½è¢«åˆ é™¤æˆ–é‡å‘½å
        result["status"] = "skipped"
        result["message"] = "File not found (may be renamed/deleted)"
        return result

    # å¦‚æœæ–‡ä»¶è¢«ç§»åŠ¨ä½†æœªè‡ªåŠ¨ä¿®å¤ï¼ŒæŠ¥å‘Šè­¦å‘Š
    if was_moved and not auto_fix_paths:
        result["status"] = "warning"
        result["message"] = f"File moved: {file_path} â†’ {actual_path}"
        return result

    # æ ¹æ®æ¨¡å¼ç±»å‹éªŒè¯
    if pattern_type == "import":
        if pattern in content:
            result["status"] = "passed"
            result["message"] = "Import statement exists"
        else:
            result["status"] = "failed"
            result["message"] = f"Missing import: {pattern}"

    elif pattern_type == "function_exists":
        if f"def {pattern}" in content:
            result["status"] = "passed"
            result["message"] = f"Function '{pattern}' exists"
        else:
            result["status"] = "failed"
            result["message"] = f"Function '{pattern}' not found"

    elif pattern_type == "contains":
        # å¯¹äºåŒ…å«æ£€æŸ¥ï¼Œä½¿ç”¨æ›´å®½æ¾çš„åŒ¹é…
        pattern_normalized = pattern.strip()
        if len(pattern_normalized) > 50:
            # é•¿æ¨¡å¼ï¼šæ£€æŸ¥å…³é”®éƒ¨åˆ†
            key_part = pattern_normalized[:50]
            if key_part in content:
                result["status"] = "passed"
                result["message"] = "Pattern found"
            else:
                result["status"] = "warning"
                result["message"] = "Pattern may have been refactored"
        else:
            if pattern_normalized in content:
                result["status"] = "passed"
                result["message"] = "Pattern found"
            else:
                result["status"] = "failed"
                result["message"] = f"Pattern not found: {pattern_normalized[:60]}..."

    else:
        result["status"] = "skipped"
        result["message"] = f"Unknown pattern type: {pattern_type}"

    return result


def update_rules_from_git(limit: int = 100, verbose: bool = True) -> dict:
    """ä» git å†å²æ›´æ–°è§„åˆ™åº“"""
    rules_data = load_rules()
    existing_ids = {r["id"] for r in rules_data["rules"]}

    # æ‰«æ git å†å²
    fix_commits = scan_git_history(limit)

    if verbose:
        print(f"ğŸ“Š æ‰«æåˆ° {len(fix_commits)} ä¸ªä¿®å¤æäº¤")

    new_rules = []
    for commit in fix_commits:
        commit_rules = analyze_commit_for_rules(commit["hash"], commit["message"])
        for rule in commit_rules:
            if rule["id"] not in existing_ids:
                new_rules.append(rule)
                existing_ids.add(rule["id"])

    if new_rules:
        rules_data["rules"].extend(new_rules)
        save_rules(rules_data)
        if verbose:
            print(f"âœ… æ–°å¢ {len(new_rules)} æ¡è§„åˆ™")
    else:
        if verbose:
            print("â„¹ï¸  æ²¡æœ‰æ–°è§„åˆ™éœ€è¦æ·»åŠ ")

    return {
        "scanned_commits": len(fix_commits),
        "new_rules": len(new_rules),
        "total_rules": len(rules_data["rules"])
    }


def validate_all_rules(verbose: bool = True, auto_fix_paths: bool = False) -> dict:
    """éªŒè¯æ‰€æœ‰è§„åˆ™"""
    rules_data = load_rules()

    results = {
        "passed": [],
        "failed": [],
        "warnings": [],
        "skipped": [],
        "moved_files": []
    }

    paths_updated = 0

    for rule in rules_data["rules"]:
        result = validate_rule(rule, auto_fix_paths)
        status = result["status"]

        if result.get("moved_to"):
            results["moved_files"].append({
                "old_path": result["file"],
                "new_path": result["moved_to"],
                "updated": result.get("path_updated", False)
            })
            if result.get("path_updated"):
                paths_updated += 1

        if status == "passed":
            results["passed"].append(result)
        elif status == "failed":
            results["failed"].append(result)
        elif status == "warning":
            results["warnings"].append(result)
        else:
            results["skipped"].append(result)

    # å¦‚æœæœ‰è·¯å¾„æ›´æ–°ï¼Œä¿å­˜è§„åˆ™
    if paths_updated > 0:
        save_rules(rules_data)
        if verbose:
            print(f"ğŸ”„ å·²è‡ªåŠ¨æ›´æ–° {paths_updated} æ¡è§„åˆ™çš„è·¯å¾„")

    if verbose:
        print(f"\n{'='*60}")
        print("ğŸ“‹ éªŒè¯ç»“æœ")
        print(f"{'='*60}")
        print(f"âœ… é€šè¿‡: {len(results['passed'])}")
        print(f"âŒ å¤±è´¥: {len(results['failed'])}")
        print(f"âš ï¸  è­¦å‘Š: {len(results['warnings'])}")
        print(f"â­ï¸  è·³è¿‡: {len(results['skipped'])}")

        # æ˜¾ç¤ºç§»åŠ¨çš„æ–‡ä»¶
        if results["moved_files"]:
            print(f"\n{'='*60}")
            print("ğŸ“‚ æ£€æµ‹åˆ°æ–‡ä»¶ç§»åŠ¨:")
            print(f"{'='*60}")
            for mf in results["moved_files"]:
                status = "âœ… å·²æ›´æ–°" if mf["updated"] else "âš ï¸ éœ€è¦æ›´æ–°"
                print(f"  {status}: {mf['old_path']} â†’ {mf['new_path']}")

        if results["failed"]:
            print(f"\n{'='*60}")
            print("âŒ å¤±è´¥è¯¦æƒ…:")
            print(f"{'='*60}")
            for r in results["failed"]:
                print(f"  [{r['commit']}] {r['file']}")
                print(f"    â†’ {r['message']}")

        if results["warnings"]:
            print(f"\n{'='*60}")
            print("âš ï¸ è­¦å‘Šè¯¦æƒ…:")
            print(f"{'='*60}")
            for r in results["warnings"]:
                print(f"  [{r['commit']}] {r['file']}")
                print(f"    â†’ {r['message']}")

    return results


def deprecate_rules(rule_ids: list, reason: str, verbose: bool = True) -> dict:
    """åºŸå¼ƒæŒ‡å®šçš„è§„åˆ™"""
    rules_data = load_rules()
    deprecated_count = 0

    for rule in rules_data["rules"]:
        if rule["id"] in rule_ids:
            rule["deprecated"] = True
            rule["deprecated_reason"] = reason
            rule["deprecated_date"] = datetime.now().isoformat()
            deprecated_count += 1

    if deprecated_count > 0:
        save_rules(rules_data)
        if verbose:
            print(f"âœ… å·²åºŸå¼ƒ {deprecated_count} æ¡è§„åˆ™")
    else:
        if verbose:
            print("âš ï¸  æœªæ‰¾åˆ°åŒ¹é…çš„è§„åˆ™")

    return {"deprecated_count": deprecated_count}


def remove_rules(rule_ids: list, verbose: bool = True) -> dict:
    """ç§»é™¤æŒ‡å®šçš„è§„åˆ™"""
    rules_data = load_rules()
    original_count = len(rules_data["rules"])

    rules_data["rules"] = [r for r in rules_data["rules"] if r["id"] not in rule_ids]

    removed_count = original_count - len(rules_data["rules"])
    if removed_count > 0:
        save_rules(rules_data)
        if verbose:
            print(f"âœ… å·²ç§»é™¤ {removed_count} æ¡è§„åˆ™")
    else:
        if verbose:
            print("âš ï¸  æœªæ‰¾åˆ°åŒ¹é…çš„è§„åˆ™")

    return {"removed_count": removed_count}


def run_full_analysis(limit: int = 100, verbose: bool = True, json_output: bool = False, auto_fix_paths: bool = False) -> dict:
    """è¿è¡Œå®Œæ•´åˆ†ææµç¨‹"""

    if verbose and not json_output:
        print("ğŸ” Smart Commit Analyzer")
        print("=" * 60)
        if auto_fix_paths:
            print("ğŸ”§ æ¨¡å¼: è‡ªåŠ¨ä¿®å¤è·¯å¾„")
        print()
        print("Step 1: ä» Git å†å²æ›´æ–°è§„åˆ™åº“...")

    # Step 1: æ›´æ–°è§„åˆ™
    update_result = update_rules_from_git(limit, verbose and not json_output)

    if verbose and not json_output:
        print()
        print("Step 2: éªŒè¯æ‰€æœ‰è§„åˆ™...")

    # Step 2: éªŒè¯è§„åˆ™
    validate_result = validate_all_rules(verbose and not json_output, auto_fix_paths)

    # ç»„åˆç»“æœ
    result = {
        "update": update_result,
        "validation": {
            "passed": len(validate_result["passed"]),
            "failed": len(validate_result["failed"]),
            "warnings": len(validate_result["warnings"]),
            "skipped": len(validate_result["skipped"]),
            "moved_files": len(validate_result["moved_files"])
        },
        "failed_details": validate_result["failed"],
        "moved_files": validate_result["moved_files"],
        "timestamp": datetime.now().isoformat()
    }

    if json_output:
        print(json.dumps(result, indent=2, ensure_ascii=False))
    elif verbose:
        print()
        print("=" * 60)
        print(f"ğŸ“Š æ€»è®¡: {update_result['total_rules']} æ¡è§„åˆ™")

        if validate_result["failed"]:
            print("âŒ æ£€æµ‹åˆ°å›å½’é£é™©ï¼è¯·æ£€æŸ¥ä¸Šè¿°å¤±è´¥é¡¹")
        else:
            print("âœ… æ‰€æœ‰è§„åˆ™éªŒè¯é€šè¿‡")

    return result


def main():
    import argparse

    parser = argparse.ArgumentParser(description="Smart Commit Analyzer - æ™ºèƒ½æäº¤åˆ†æå™¨")
    parser.add_argument("--update", action="store_true", help="åªæ›´æ–°è§„åˆ™åº“")
    parser.add_argument("--validate", action="store_true", help="åªéªŒè¯è§„åˆ™")
    parser.add_argument("--commits", type=int, default=100, help="æ‰«ææäº¤æ•°é‡")
    parser.add_argument("--json", action="store_true", help="JSON è¾“å‡º")
    parser.add_argument("--show-rules", action="store_true", help="æ˜¾ç¤ºæ‰€æœ‰è§„åˆ™")
    parser.add_argument("--fix-paths", action="store_true", help="è‡ªåŠ¨ä¿®å¤ç§»åŠ¨æ–‡ä»¶çš„è·¯å¾„")
    parser.add_argument("--deprecate", nargs="+", metavar="RULE_ID", help="åºŸå¼ƒæŒ‡å®šè§„åˆ™ (ç©ºæ ¼åˆ†éš”å¤šä¸ªID)")
    parser.add_argument("--remove", nargs="+", metavar="RULE_ID", help="ç§»é™¤æŒ‡å®šè§„åˆ™ (ç©ºæ ¼åˆ†éš”å¤šä¸ªID)")
    parser.add_argument("--reason", type=str, default="Obsolete", help="åºŸå¼ƒåŸå›  (ä¸ --deprecate é…åˆ)")

    args = parser.parse_args()

    # å¤„ç†åºŸå¼ƒè§„åˆ™
    if args.deprecate:
        deprecate_rules(args.deprecate, args.reason, not args.json)
        return

    # å¤„ç†ç§»é™¤è§„åˆ™
    if args.remove:
        remove_rules(args.remove, not args.json)
        return

    if args.show_rules:
        rules_data = load_rules()
        if args.json:
            print(json.dumps(rules_data, indent=2, ensure_ascii=False))
        else:
            print(f"ğŸ“‹ è§„åˆ™åˆ—è¡¨ ({len(rules_data['rules'])} æ¡)")
            print("=" * 60)
            for rule in rules_data["rules"]:
                print(f"[{rule['id']}] {rule['commit']} - {rule['file']}")
                print(f"    {rule['message'][:60]}")
                print(f"    Pattern: {rule['pattern'][:50]}...")
                print()
        return

    if args.update:
        update_rules_from_git(args.commits, not args.json)
    elif args.validate:
        validate_all_rules(not args.json, args.fix_paths)
    else:
        run_full_analysis(args.commits, True, args.json, args.fix_paths)


if __name__ == "__main__":
    main()
